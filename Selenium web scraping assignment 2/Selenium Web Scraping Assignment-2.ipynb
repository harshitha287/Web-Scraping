{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Web Scraping Assignment-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Driver [C:\\Users\\Devu\\.wdm\\drivers\\chromedriver\\win32\\91.0.4472.101\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import time\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_page=driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert=driver.find_element_by_xpath(\"//div[@class='sWrap']/div//input[@class='sugInp']\")\n",
    "insert.send_keys(\"Data Analyst\")\n",
    "\n",
    "insert1=driver.find_element_by_xpath(\"//div[2]/div//input[@class='sugInp']\")\n",
    "insert1.send_keys(\"Bangalore\")\n",
    "\n",
    "\n",
    "insert2=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "insert2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "company_name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "experience_req=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "\n",
    "print(len(job_title),len(job_location),len(company_name),len(experience_req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Job_Title': [], 'Job_Location': [], 'Company_Name': [], 'Experience_Req': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_Analyst={}\n",
    "Data_Analyst['Job_Title']=[]\n",
    "Data_Analyst['Job_Location']=[]\n",
    "Data_Analyst['Company_Name']=[]\n",
    "Data_Analyst['Experience_Req']=[]\n",
    "Data_Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i in job_title:\n",
    "    Data_Analyst['Job_Title'].append(i.text)\n",
    "print(len(Data_Analyst['Job_Title']))\n",
    "\n",
    "for k in job_location:\n",
    "    Data_Analyst['Job_Location'].append(k.text)\n",
    "print(len(Data_Analyst['Job_Location']))\n",
    "\n",
    "for j in company_name:\n",
    "    Data_Analyst['Company_Name'].append(j.text)\n",
    "print(len(Data_Analyst['Company_Name']))\n",
    "\n",
    "for h in experience_req:\n",
    "    Data_Analyst['Experience_Req'].append(h.text)\n",
    "print(len(Data_Analyst['Experience_Req']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Req</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst - Data Lineage</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Opex Global Services</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Informatica MDM</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data analysts</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excel VBA Jobs Bangalore | VBA data analyst Jobs</td>\n",
       "      <td>Mysore/Mysuru, Coimbatore, Bangalore/Bengaluru</td>\n",
       "      <td>Mind Circus Innovation</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst (2positions)//immediate Joiners//...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business/Data Analyst - SSE/LA</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CGI Information Systems and Management Consult...</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>dotSolved India Pvt., Ltd.</td>\n",
       "      <td>9-13 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - Category Demand Management (Rev...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0                 Senior Data Analyst - Data Lineage   \n",
       "1                     Data Analyst - Informatica MDM   \n",
       "2                                      Data analysts   \n",
       "3   Excel VBA Jobs Bangalore | VBA data analyst Jobs   \n",
       "4  Data Analyst (2positions)//immediate Joiners//...   \n",
       "5                     Business/Data Analyst - SSE/LA   \n",
       "6                              Business Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                                       Data Analyst   \n",
       "9  Data Analyst - Category Demand Management (Rev...   \n",
       "\n",
       "                                       Job_Location  \\\n",
       "0                               Bangalore/Bengaluru   \n",
       "1                               Bangalore/Bengaluru   \n",
       "2                               Bangalore/Bengaluru   \n",
       "3    Mysore/Mysuru, Coimbatore, Bangalore/Bengaluru   \n",
       "4                               Bangalore/Bengaluru   \n",
       "5                               Bangalore/Bengaluru   \n",
       "6  Chennai, Bangalore/Bengaluru, Mumbai (All Areas)   \n",
       "7                               Bangalore/Bengaluru   \n",
       "8                               Bangalore/Bengaluru   \n",
       "9                               Bangalore/Bengaluru   \n",
       "\n",
       "                                        Company_Name Experience_Req  \n",
       "0                               Opex Global Services       5-10 Yrs  \n",
       "1                Shell India Markets Private Limited        6-9 Yrs  \n",
       "2                             IBM India Pvt. Limited        3-5 Yrs  \n",
       "3                             Mind Circus Innovation        0-1 Yrs  \n",
       "4                                 Tech Mahindra Ltd.        4-8 Yrs  \n",
       "5  CGI Information Systems and Management Consult...        2-5 Yrs  \n",
       "6                         dotSolved India Pvt., Ltd.       9-13 Yrs  \n",
       "7                           Myntra Designs Pvt. Ltd.        3-5 Yrs  \n",
       "8                           Myntra Designs Pvt. Ltd.        3-8 Yrs  \n",
       "9                           Myntra Designs Pvt. Ltd.        1-4 Yrs  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(Data_Analyst)\n",
    "df=df.iloc[0:10]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data_Analyst.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing URL of Naukri Website.\n",
    "url = \"https://www.naukri.com/data-scientist-jobs-in-bangalore?k=data%20scientist&l=bangalore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting URL through Webdriver.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty Sets.\n",
    "Job_title = []\n",
    "Job_location = []\n",
    "Company_name = []\n",
    "Job_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"f7c7e731-ed2e-463b-8892-b05f20e150cb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"07d06a1a-9389-4eaf-b4f5-8ef46b06b935\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"fc03b6ce-13dd-4f97-ae2d-6d78d2cb98e0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"39aca4c7-753f-4c43-91ff-3359d4abdecc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"1a9ee2ff-918e-4d18-92b1-494cd464bf57\")>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the elements for titles.\n",
    "titles = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Data Scientist',\n",
       " 'Immediate job opening - Senior Data Scientist',\n",
       " 'Senior Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Data analytics / Data scientist intern (work from Home)',\n",
       " 'Data Scientist - IBM Garage',\n",
       " 'Deputy Manager - Datalabs (Data Scientist)',\n",
       " 'Digital Enterprise Architect (Data Scientist)',\n",
       " 'Data Scientist']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Job_title through appending the text.\n",
    "for i in titles:\n",
    "    title=i.text\n",
    "    Job_title.append(title)\n",
    "Job_title[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"a0ce371a-51bf-4506-bc7c-cb2b4b9a93c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"e858add2-a177-45d1-8a8e-d97e61065abb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"2c04b785-6fc7-419e-b897-a911edd1ae1f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"0439e3de-3ecf-48ac-8647-d70b1ebac3a0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"70ee9b3e-b7e1-4e7f-aa26-03f79c3d9d82\")>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the elements for locations.\n",
    "locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai (All Areas)',\n",
       " 'Kolkata, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'Bengaluru/Bangalore',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Job_location through appending the text.\n",
    "for i in locations:\n",
    "    location=i.text\n",
    "    Job_location.append(location)\n",
    "Job_location[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"4ad819d9-264d-48f4-acd0-5cd7f9f35133\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"dfdd73d5-c38d-4c6d-8477-7226745bfc88\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"6d83805a-b347-4c05-9980-6fe44d179975\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"772c654d-6817-405c-8c81-fb43e1768dc8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"0da285051ea499fa49051ab6c468ecdb\", element=\"3194e3c9-b2b1-4287-b82e-70b9081cc2a9\")>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the elements for Companies.\n",
    "companies = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RedBus',\n",
       " 'RedBus',\n",
       " 'Fractal Analytics',\n",
       " 'FICO',\n",
       " 'Fractal Analytics',\n",
       " 'TalkValley LLC',\n",
       " 'IBM India Pvt. Limited',\n",
       " 'HDFC LIFE INSURANCE COMPANY LIMITED',\n",
       " 'Mphasis Limited',\n",
       " 'IBM India Pvt. Limited']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Company_name through appending the text.\n",
    "for i in companies:\n",
    "    company=i.text\n",
    "    Company_name.append(company)\n",
    "Company_name[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the elements for Job_description and appending text.\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "urls=[i.get_attribute(\"href\")for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")]\n",
    "for url in urls[0:10]:\n",
    "    try:\n",
    "        \n",
    "        driver.get(url)\n",
    "        raw_description=driver.find_element_by_xpath(\"//section[@class='job-desc']/div[1]\").text\n",
    "        description=raw_description.replace(\"Contact Person\",\"@@@@@\")\n",
    "        description= description.split(\"@@@@@\")\n",
    "        Job_description.append(description[0])\n",
    "    except NoSuchElementException :\n",
    "        Job_description.append(\"---\")\n",
    "# Reference taken from github that,to use in a way of for loop to get all urls of different job descriptions in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---',\n",
       " '---',\n",
       " 'Serve as primary technical lead on all phases of the projects from providing solutioning, experimentation and deployment.\\nWilling to get your hands dirty with data analysis as well as be comfortable delegating tasks to junior members on the project team, thus providing technical guidance and oversight to the overall project.\\nHands-on solutioning within the chosen AI platform to partner with both customer and internal data solution architects.\\nMust Have\\n8 years of experience delivering results from advanced analytics projects; with at least 2 years experience leading a project or large workstream\\nExperience working on data analytics problems in commercial problems space such as pricing, supply chain, marketing or customer experience. Proven track record with building and deploying supervised classification, regression, deep learning and unsupervised clustering models and time series analysis.\\n5 years of Knowledge and or experience with data transformations using SQL, leveraging SAS or SAS Viya platform.\\nStatistical model building with 5 years of experience\\nBasic understanding of machine learning models development in python and leveraging AWS.\\nBachelors Degree in Engineering, Mathematics Analytics, Economics or Business\\nNice to Have\\n5 years of Knowledge and experience with SAS or SAS Viya platform\\nExperience working with remote teams spread across the globe\\nMachine Learning modelling experience\\nPython programming to build ML models\\nAWS or equivalent other cloud based services knowledge SAS EG\\nExperience working in a data analytics COE or in an analytics consulting company',\n",
       " 'Job Summary\\nThe candidate should be a Cybersecurity Data Scientist with a background in machine learning for cybersecurity applications.\\nHeShe will contribute, provide subject matter expertise in the area of cybersecurity for critical infrastructure systems.\\n\\nResearch, develop, design and implement machine learning algorithms for cyber threat detection in operational technology environments, under limited direction.\\nIdentify data types to enable detection of cyber events.\\nTest and validate developed algorithms on real operational data.\\nIdentify, define, and scope complex data analytics problems in the cybersecurity domain.\\nDevelop cross-domain strategies for increased network security and resiliency of the entire network.',\n",
       " 'Position Description:\\nThe Artificial Intelligence and Machine Learning (AIML) group at Fractal Analytics is actively involved in helping Fortune 500 companies by enabling them to discover how they can leverage their data using advanced and sophisticated AI/ML algorithms for which we are looking for Data Scientists with the capability to work on independent statistical and machine learning research/ projects. If you are a problem solver with a curiosity for exploring new techniques and technologies in AIML space, then we would like to talk with you.\\nJob Responsibilities:\\nAbility to understand a problem statement and implement analytical solutions & techniques independently with independently/proactively/thought-leadership\\nWork with stakeholders throughout the organization to identify opportunities for leveraging company/client data to drive business solutions\\nFast learner: ability to learn and pick up a new language/tool/ platform quickly\\nConceptualize, design, and deliver high-quality solutions and insightful analysis\\nConduct research and prototyping innovations; data and requirements gathering; solution scoping and architecture; consulting clients and client facing teams on advanced statistical and machine learning problems\\nCollaborate and coordinate with different functional teams (engineering and product development) to implement models and monitor outcomes\\nAbility to deliver AIML based solutions around a host of domains and problems, with some of them being: Customer Segmentation & Targeting, Propensity Modeling, Churn Modeling, Lifetime Value Estimation, Forecasting, Recommender Systems, Modeling Response to Incentives, Marketing Mix Optimization, Price Optimization\\nExperience Required:\\nMinimum 10+ years of experience in Machine Learning domain\\nExpert level proficiency in at least one of R and Python\\nAbility to create efficient solutions to complex problems. Strong skills in data-structures and ML algorithms\\nExperience of working on end-to-end data science pipeline: problem scoping, data gathering, EDA, modelling, insights, visualizations, monitoring and maintenance\\nProblem-solving: Ability to break the problem into small parts and applying relevant techniques to drive required outcomes\\nIntermediate to advanced knowledge of machine learning, probability theory, statistics, and algorithms. You will be required to discuss and use various algorithms and approaches on a daily basis\\nWe use regression, Bayesian methods, tree-based learners, SVM, RF, XGBOOST, time series modeling, dimensionality reduction, SEM, GLM, GLMM, clustering, Deep learning etc. on a regular basis. If you know few of them you are good to go\\nGood to Have:\\nExperience in one of the upcoming technologies like deep learning, NLP, image processing, recommender systems\\nExperience of working in on one or more domains:\\nCPG: pricing and promotion analytics, marketing analytics, trade promotions, supply chain management\\nBFSI: cross-sell, up-sell, campaign analytics, treasury analytics, fraud detection\\nHealthcare: medical adherence, medical risk profiling, EHR data, fraud-waste-abuse\\nExperience in working with Linux computing environment and use of command line tools like sed/awk\\nGood grasp on databases including RDBMS, NoSQL, MongoDB etc.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Job_description.\n",
    "Job_description[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of Data_scientist_jobs.\n",
    "Data_Scientist_Jobs = pd.DataFrame({})\n",
    "Data_Scientist_Jobs['Job_Title'] = Job_title[:10]\n",
    "Data_Scientist_Jobs['Job_Location'] = Job_location[:10]\n",
    "Data_Scientist_Jobs['Company_Name'] = Company_name[:10]\n",
    "Data_Scientist_Jobs['Job_Description'] = Job_description[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>RedBus</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immediate job opening - Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>RedBus</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Serve as primary technical lead on all phases ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>FICO</td>\n",
       "      <td>Job Summary\\nThe candidate should be a Cyberse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Position Description:\\nThe Artificial Intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data analytics / Data scientist intern (work f...</td>\n",
       "      <td>Kolkata, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>TalkValley LLC</td>\n",
       "      <td>We are a group of tenured professors from Tier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - IBM Garage</td>\n",
       "      <td>Bengaluru/Bangalore</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deputy Manager - Datalabs (Data Scientist)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>HDFC LIFE INSURANCE COMPANY LIMITED</td>\n",
       "      <td>Roles &amp; Responsibilities\\nDevelop end to end s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Digital Enterprise Architect (Data Scientist)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Mphasis Limited</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1      Immediate job opening - Senior Data Scientist   \n",
       "2                              Senior Data Scientist   \n",
       "3                                Lead Data Scientist   \n",
       "4                                Lead Data Scientist   \n",
       "5  Data analytics / Data scientist intern (work f...   \n",
       "6                        Data Scientist - IBM Garage   \n",
       "7         Deputy Manager - Datalabs (Data Scientist)   \n",
       "8      Digital Enterprise Architect (Data Scientist)   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                        Job_Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2      Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...   \n",
       "5          Kolkata, Bangalore/Bengaluru, Delhi / NCR   \n",
       "6                                Bengaluru/Bangalore   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                          Company_Name  \\\n",
       "0                               RedBus   \n",
       "1                               RedBus   \n",
       "2                    Fractal Analytics   \n",
       "3                                 FICO   \n",
       "4                    Fractal Analytics   \n",
       "5                       TalkValley LLC   \n",
       "6               IBM India Pvt. Limited   \n",
       "7  HDFC LIFE INSURANCE COMPANY LIMITED   \n",
       "8                      Mphasis Limited   \n",
       "9               IBM India Pvt. Limited   \n",
       "\n",
       "                                     Job_Description  \n",
       "0                                                ---  \n",
       "1                                                ---  \n",
       "2  Serve as primary technical lead on all phases ...  \n",
       "3  Job Summary\\nThe candidate should be a Cyberse...  \n",
       "4  Position Description:\\nThe Artificial Intellig...  \n",
       "5  We are a group of tenured professors from Tier...  \n",
       "6                                                ---  \n",
       "7  Roles & Responsibilities\\nDevelop end to end s...  \n",
       "8                                                ---  \n",
       "9                                                ---  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final DataFrame.\n",
    "Data_Scientist_Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_scientist_jobs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage to to scrape data for “Data Scientist” designation for first 10 job results along with their respective job-title, job-location, company_name, experience_required\n",
    "1. The location filter to be used is “Delhi/NCR”\n",
    "2. The salary filter to be used is “3-6” lakhs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field\n",
    "3. Then click the search button\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Naukri website after searching.\n",
    "url = 'https://www.naukri.com/data-scientist-jobs-in-delhi-ncr?k=data%20scientist&l=delhi%2Fncr&ctcFilter=3to6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting URL from webdriver.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty Sets.\n",
    "Job_title = []\n",
    "Job_location = []\n",
    "Company_name = []\n",
    "Experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"dc778864-f6ba-419e-8148-f070cbdd8e5a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"fad0ec42-4513-4944-aadc-c54b98f6b761\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"6b34148b-edd2-4106-b799-8805a97ab587\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"c8e98034-3ac9-4dc3-a90b-ee746315630c\")>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding elements of titles from xpath.\n",
    "titles = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"e050bfe7-f82e-45fe-bf01-5d2e37c020ab\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"a7d476a3-5037-464f-a864-32c635358409\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"b81cfc08-7f05-4e47-8a9b-621293ca36ec\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"b0dbd334-0543-4fb4-b427-3e347cc5317a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"ca814a82-b752-42b1-aec8-bf1105784324\")>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding elements of locations by xpath.\n",
    "locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"91c2fe88-8690-4c1b-89c0-d92daaf4d0a0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"e3eb5c09-3d73-4591-859f-2b31d2a5811d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"d1b09ac2-3994-4712-9ac3-0abc584965c1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"a7aebe2c-ca5a-4fed-94bf-e41009412474\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"4b5eb6c2-ddc6-467c-b252-a11e87e80a1b\")>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding elements of companies through xpath of html code.\n",
    "companies = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"c3e07b09-7faa-4ae1-b637-d7e6910e1985\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"6df3ae06-20b3-46d6-9276-2eb1f53b0cd8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"e7b00156-2f4d-4af7-9571-2a259c23734a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"ea5d493d-dcf8-409c-852f-9d8a63d6a16a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f14c0952f560c0c92427701343aea895\", element=\"238668b8-a9c9-4504-9123-f4b2098dca01\")>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding elements of experiences through xpath.\n",
    "experiences = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experiences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kolkata, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru',\n",
       " 'New Delhi',\n",
       " 'Noida(Sector-59 Noida)',\n",
       " 'Noida, Gurgaon/Gurugram, Delhi / NCR',\n",
       " 'Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram, Chennai, Bangalore/Bengaluru',\n",
       " 'Gurgaon/Gurugram, Delhi / NCR',\n",
       " 'Noida',\n",
       " 'Gurgaon/Gurugram',\n",
       " 'Noida, Gurgaon/Gurugram']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using for loop and appending the text into empty set Job_location to get Job_location.\n",
    "for i in locations:\n",
    "    location = i.text\n",
    "    Job_location.append(location)\n",
    "Job_location[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data analytics / Data scientist intern (work from Home)',\n",
       " 'Data Scientist',\n",
       " 'Chaayos is Looking For Data Scientist',\n",
       " 'Junior Data Scientist',\n",
       " 'We are hiring- Data Scientist +Python- Noida',\n",
       " 'Data Scientist / Data Analyst -Business Analyst',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Analyst/Scientist Big Data, Statistical Techniques',\n",
       " 'Business Analyst- Data Scientist']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using for loop and appending the text into empty set Job_title.\n",
    "for i in titles:\n",
    "    Job_title.append(i.text)\n",
    "Job_title[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TalkValley LLC',\n",
       " 'Fractal Analytics',\n",
       " 'Chaayos (Sunshine Teahouse Pvt. Ltd.)',\n",
       " 'R Systems International Ltd.',\n",
       " 'RANDSTAD INDIA PVT LTD',\n",
       " 'Inflexion Analytix Private Limited',\n",
       " 'Milliman India Pvt Ltd',\n",
       " 'NEC CORPORATION INDIA PRIVATE LIMITED',\n",
       " 'The Search House (A Div of JSD Search House Pvt. L td.)',\n",
       " 'Wipro']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using for loop and appending the text into empty set company_name.\n",
    "for i in companies:\n",
    "    company = i.text\n",
    "    Company_name.append(company)\n",
    "Company_name[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-5 Yrs',\n",
       " '3-7 Yrs',\n",
       " '0-5 Yrs',\n",
       " '3-5 Yrs',\n",
       " '4-7 Yrs',\n",
       " '0-3 Yrs',\n",
       " '2-5 Yrs',\n",
       " '3-8 Yrs',\n",
       " '2-7 Yrs',\n",
       " '2-5 Yrs']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using for loop and appending the text into empty set Experience_required.\n",
    "for i in experiences:\n",
    "    experience = i.text\n",
    "    Experience_required.append(experience)\n",
    "Experience_required[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of Jobs.\n",
    "Jobs = pd.DataFrame({})\n",
    "Jobs['Title'] = Job_title[:10]\n",
    "Jobs['Location'] = Job_location[:10]\n",
    "Jobs['Company_Name'] = Company_name[:10]\n",
    "Jobs['Experience_Required'] = Experience_required[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data analytics / Data scientist intern (work f...</td>\n",
       "      <td>Kolkata, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>TalkValley LLC</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chaayos is Looking For Data Scientist</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Chaayos (Sunshine Teahouse Pvt. Ltd.)</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Noida(Sector-59 Noida)</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We are hiring- Data Scientist +Python- Noida</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Milliman India Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>NEC CORPORATION INDIA PRIVATE LIMITED</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst/Scientist Big Data, Statistical T...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>The Search House (A Div of JSD Search House Pv...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business Analyst- Data Scientist</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Data analytics / Data scientist intern (work f...   \n",
       "1                                     Data Scientist   \n",
       "2              Chaayos is Looking For Data Scientist   \n",
       "3                              Junior Data Scientist   \n",
       "4       We are hiring- Data Scientist +Python- Noida   \n",
       "5    Data Scientist / Data Analyst -Business Analyst   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8  Data Analyst/Scientist Big Data, Statistical T...   \n",
       "9                   Business Analyst- Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0          Kolkata, Bangalore/Bengaluru, Delhi / NCR   \n",
       "1      Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "2                                          New Delhi   \n",
       "3                             Noida(Sector-59 Noida)   \n",
       "4               Noida, Gurgaon/Gurugram, Delhi / NCR   \n",
       "5  Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...   \n",
       "6                      Gurgaon/Gurugram, Delhi / NCR   \n",
       "7                                              Noida   \n",
       "8                                   Gurgaon/Gurugram   \n",
       "9                            Noida, Gurgaon/Gurugram   \n",
       "\n",
       "                                        Company_Name Experience_Required  \n",
       "0                                     TalkValley LLC             0-5 Yrs  \n",
       "1                                  Fractal Analytics             3-7 Yrs  \n",
       "2              Chaayos (Sunshine Teahouse Pvt. Ltd.)             0-5 Yrs  \n",
       "3                       R Systems International Ltd.             3-5 Yrs  \n",
       "4                             RANDSTAD INDIA PVT LTD             4-7 Yrs  \n",
       "5                 Inflexion Analytix Private Limited             0-3 Yrs  \n",
       "6                             Milliman India Pvt Ltd             2-5 Yrs  \n",
       "7              NEC CORPORATION INDIA PRIVATE LIMITED             3-8 Yrs  \n",
       "8  The Search House (A Div of JSD Search House Pv...             2-7 Yrs  \n",
       "9                                              Wipro             2-5 Yrs  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final DataFrame.\n",
    "Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"jobs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Write a python program to scrape data for first 10 job results for Data scientist with designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field\n",
    "3. Then click the search button\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Glassdoor Website.\n",
    "url = 'https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button\n",
    "4. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company\n",
    "5.Store the data in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm?clickSource=searchBtn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting URL from webdriver.chrome.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Flipkart Website.\n",
    "url =  'https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting into the URL by driver. \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can open with search button code but it was asking login.so,We used direct method to get into page url.\n",
    "# Creating empty sets to append.\n",
    "Brand = []\n",
    "Product_description = []\n",
    "Price = []\n",
    "Discount = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding elements of three pages by xpath and appending text.\n",
    "for i in range(0,3):\n",
    "    brand = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    description = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for i in description:\n",
    "        Product_description.append(i.text)\n",
    "        \n",
    "    price = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    discount = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span[1]\")\n",
    "    for i in discount:\n",
    "        Discount.append(i.text)\n",
    "    driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PIRASO',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'Singco',\n",
       " 'PIRASO',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Fastrack',\n",
       " 'Singco India',\n",
       " 'PIRASO']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 values of Brand variable.\n",
    "Brand[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹237',\n",
       " '₹541',\n",
       " '₹621',\n",
       " '₹299',\n",
       " '₹249',\n",
       " '₹237',\n",
       " '₹404',\n",
       " '₹733',\n",
       " '₹209',\n",
       " '₹331']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 values of Price variable.\n",
    "Price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UV Protection Aviator Sunglasses (54)',\n",
       " 'UV Protection Rectangular Sunglasses (Free Size)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'Mirrored, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'Mirrored Aviator Sunglasses (53)',\n",
       " 'UV Protection Aviator Sunglasses (54)',\n",
       " 'UV Protection, Gradient Rectangular Sunglasses (Free Si...',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Round Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (58)']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 values of Product_descripition variable.\n",
    "Product_description[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['85% off',\n",
       " '₹258 off',\n",
       " '₹178 off',\n",
       " '88% off',\n",
       " '75% off',\n",
       " '85% off',\n",
       " '79% off',\n",
       " '₹166 off',\n",
       " '79% off',\n",
       " '87% off']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 values of Discount variable.\n",
    "Discount[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PIRASO',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'Singco',\n",
       " 'PIRASO',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Fastrack',\n",
       " 'Singco India',\n",
       " 'PIRASO',\n",
       " 'DEIXELS',\n",
       " 'GANSTA',\n",
       " 'Fastrack',\n",
       " 'PIRASO',\n",
       " 'NuVew',\n",
       " 'HIPPON',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Silver Kartz',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'PHENOMENAL',\n",
       " 'PHENOMENAL',\n",
       " 'Singco India',\n",
       " 'PIRASO',\n",
       " 'ROYAL SON',\n",
       " 'ROYAL SON',\n",
       " 'Singco India',\n",
       " 'Singco India',\n",
       " 'Fastrack',\n",
       " 'NuVew',\n",
       " 'ROYAL SON',\n",
       " 'hipe',\n",
       " 'Fastrack',\n",
       " 'Fravy',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'NuVew',\n",
       " 'ROYAL SON',\n",
       " 'GANSTA',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'elegante',\n",
       " 'PIRASO',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'Singco',\n",
       " 'PIRASO',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Fastrack',\n",
       " 'Singco India',\n",
       " 'PIRASO',\n",
       " 'DEIXELS',\n",
       " 'GANSTA',\n",
       " 'Fastrack',\n",
       " 'PIRASO',\n",
       " 'NuVew',\n",
       " 'HIPPON',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Silver Kartz',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'PHENOMENAL',\n",
       " 'PHENOMENAL',\n",
       " 'Singco India',\n",
       " 'PIRASO',\n",
       " 'ROYAL SON',\n",
       " 'ROYAL SON',\n",
       " 'Singco India',\n",
       " 'Singco India',\n",
       " 'Fastrack',\n",
       " 'NuVew',\n",
       " 'ROYAL SON',\n",
       " 'hipe',\n",
       " 'Fastrack',\n",
       " 'Fravy',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'NuVew',\n",
       " 'ROYAL SON',\n",
       " 'GANSTA',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'elegante',\n",
       " 'PIRASO',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'Singco',\n",
       " 'PIRASO',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Fastrack',\n",
       " 'Singco India',\n",
       " 'PIRASO',\n",
       " 'DEIXELS',\n",
       " 'GANSTA',\n",
       " 'Fastrack',\n",
       " 'PIRASO',\n",
       " 'NuVew',\n",
       " 'HIPPON',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Silver Kartz',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'PHENOMENAL',\n",
       " 'PHENOMENAL',\n",
       " 'Singco India',\n",
       " 'PIRASO',\n",
       " 'ROYAL SON',\n",
       " 'ROYAL SON',\n",
       " 'Singco India',\n",
       " 'Singco India',\n",
       " 'Fastrack',\n",
       " 'NuVew',\n",
       " 'ROYAL SON',\n",
       " 'hipe',\n",
       " 'Fastrack',\n",
       " 'Fravy',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'NuVew',\n",
       " 'ROYAL SON',\n",
       " 'GANSTA',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'elegante']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Brand variable.\n",
    "Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame.\n",
    "Sun_Glasses = pd.DataFrame({})\n",
    "Sun_Glasses['Brand'] = Brand[:100]\n",
    "Sun_Glasses['Product_Description'] = Product_description[:100]\n",
    "Sun_Glasses['Price'] = Price[:100]\n",
    "Sun_Glasses['Discount'] = Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120\n"
     ]
    }
   ],
   "source": [
    "# Lengths of the all created variables.\n",
    "print(len(Product_description),len(Price),len(Brand),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹237</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹541</td>\n",
       "      <td>₹258 off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹621</td>\n",
       "      <td>₹178 off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹299</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singco</td>\n",
       "      <td>Mirrored Aviator Sunglasses (53)</td>\n",
       "      <td>₹249</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>HIPPON</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (55)</td>\n",
       "      <td>₹251</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹246</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product_Description Price  \\\n",
       "0           PIRASO              UV Protection Aviator Sunglasses (54)  ₹237   \n",
       "1         Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹541   \n",
       "2         Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹621   \n",
       "3   kingsunglasses  Mirrored, UV Protection Wayfarer Sunglasses (F...  ₹299   \n",
       "4           Singco                   Mirrored Aviator Sunglasses (53)  ₹249   \n",
       "..             ...                                                ...   ...   \n",
       "95          HIPPON             UV Protection Wayfarer Sunglasses (55)  ₹251   \n",
       "96  ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ₹499   \n",
       "97    Silver Kartz      UV Protection Wayfarer Sunglasses (Free Size)  ₹246   \n",
       "98  ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)  ₹499   \n",
       "99      PHENOMENAL  UV Protection Retro Square Sunglasses (Free Size)  ₹399   \n",
       "\n",
       "    Discount  \n",
       "0    85% off  \n",
       "1   ₹258 off  \n",
       "2   ₹178 off  \n",
       "3    88% off  \n",
       "4    75% off  \n",
       "..       ...  \n",
       "95   79% off  \n",
       "96   77% off  \n",
       "97   83% off  \n",
       "98   77% off  \n",
       "99   80% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final DataFrame.\n",
    "Sun_Glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"flipkart_sunglasses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rating \n",
    "2. Review_summary \n",
    "3. Full review\n",
    "- You have to scrape this data for first 100 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Flipkart website.\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty sets for Defined variables.\n",
    "Rating = []\n",
    "Review_summary = []\n",
    "Full_review = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the button through driver by xpath.\n",
    "Change_button = driver.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking the change_button.\n",
    "Change_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding elements of rating,review_summary and full_review through driver by xpath.\n",
    "# Using for loop and appending the text function to all defined variables.\n",
    "for i in range(0,10):\n",
    "    rating = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for i in rating:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    review_summary = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for i in review_summary:\n",
    "        Review_summary.append(i.text)\n",
    "        \n",
    "    full_review = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for i in full_review:\n",
    "        Full_review.append(i.text)\n",
    "    driver.find_element_by_xpath(\"//a[@class='_1LKTO3']/span[1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '5', '5', '5', '5', '4', '5', '5', '5', '5']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking rating of first 10 values.\n",
    "Rating[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brilliant',\n",
       " 'Perfect product!',\n",
       " 'Great product',\n",
       " 'Worth every penny',\n",
       " 'Fabulous!',\n",
       " 'Good choice',\n",
       " 'Highly recommended',\n",
       " 'Perfect product!',\n",
       " 'Perfect product!',\n",
       " 'Worth every penny']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking review_summary of first 10 values.\n",
    "Review_summary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Best Phone for the Money\\n\\nThe iPhone 11 offers superb cameras, a more durable design and excellent battery life for an affordable price.\\n\\nCompelling ultra-wide camera\\nNew Night mode is excellent\\nLong battery life',\n",
       " 'Amazing phone with great cameras and better battery which gives you the best performance. I just love the camera .',\n",
       " 'Amazing Powerful and Durable Gadget.\\n\\nI’m am very happy with the camera picture quality, Amazing face id unlocked in dark room, Strong battery with perfect screen size as you can carry easily in pocket. This is my third iPhone.\\n\\nI shifted from android Samsung Note series to iPhone because of the strong build quality and peace of mind for next 3-4 years.\\n\\nDon’t think to much just go for it and I suggest you to go for minimum 128gb variant or more 256gb.\\n\\nI’ve attached my puppy pics and no fi...\\nREAD MORE',\n",
       " 'Previously I was using one plus 3t it was a great phone\\nAnd then I decided to upgrade I am stuck between Samsung s10 plus or iPhone 11\\nI have seen the specs and everything were good except the display it’s somewhere between 720-1080 and it’s not even an amoled it’s an LCD display\\nBut I decided to go with iPhone because I have never used an IOS device I have Been an android user from the past 9 years I ordered IPhone 11 (128gb) product red\\nMy experience after using 3 weeks\\n1. The delivery ...\\nREAD MORE',\n",
       " 'This is my first iOS phone. I am very happy with this product. Very much satisfied with this. I love this phone.',\n",
       " 'So far it’s been an AMAZING experience coming back to iOS after nearly a decade but it’s not as versatile as android though phone is sturdy dropped it accidentally a couple of times and nothing happened fortunately camera is awesome',\n",
       " 'iphone 11 is a very good phone to buy only if you can compromise for the display. The display on this is device is pretty good but you can get other options with better displays in this price segment.\\nIf you can survive with an HD+ LCD panel with thicker bezels and a notch up top then this is a very good phone for you.\\nCameras are awesome, battery backup excellent, great performance and a decent premium look. Good job Apple !',\n",
       " 'It’s a must buy who is looking for an upgrade from previous generation of iPhones. If you are using XR then still you can hold on for sometime and upgrade to 2020 model else this phone is a must buy . Camera quality is amazing and wide angle is something to count upon. Performance wise it’s amazing and feels premium while holding in hand. So a big YES for this device. Go for 128 GB variant as the 4K videos will occupy lots of space and the storage can get over very quickly. Try to buy it with...\\nREAD MORE',\n",
       " 'Value for money❤️❤️\\nIts awesome mobile phone in the world ...\\nDisplay was very good and bright ..\\nTrust me freinds you r never regret after Buying..\\nJust go for it....\\nI love this phone and i switch to iphone x to 11',\n",
       " 'Best budget Iphone till date ❤️ go for it guys without second thought. Let me explain you guys about Camera, Display, battery, and performance.\\n\\nCamera: at this price range there is no comparison of camera, you’ll love the picture quality as well as video quality. I am a Vlogger I wanted an iPhone with 4k video by front camera and I got this phone and I am more than happy 😃\\n\\nBattery: I use this phone roughly as I am active social media person and I have a youtube channel. so the battery ba...\\nREAD MORE']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking full_review of first 10 values.\n",
    "Full_review[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of Apple_Ratings.\n",
    "Apple_Ratings = pd.DataFrame({})\n",
    "Apple_Ratings['Ratings'] = Rating\n",
    "Apple_Ratings['Review_Summary'] = Review_summary\n",
    "Apple_Ratings['Full_Review'] = Full_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Review_Summary</th>\n",
       "      <th>Full_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️\\nIts awesome mobile phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings      Review_Summary  \\\n",
       "0        5           Brilliant   \n",
       "1        5    Perfect product!   \n",
       "2        5       Great product   \n",
       "3        5   Worth every penny   \n",
       "4        5           Fabulous!   \n",
       "..     ...                 ...   \n",
       "95       4         Good choice   \n",
       "96       5  Highly recommended   \n",
       "97       5    Perfect product!   \n",
       "98       5    Perfect product!   \n",
       "99       5   Worth every penny   \n",
       "\n",
       "                                          Full_Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "3   Previously I was using one plus 3t it was a gr...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  So far it’s been an AMAZING experience coming ...  \n",
       "96  iphone 11 is a very good phone to buy only if ...  \n",
       "97  It’s a must buy who is looking for an upgrade ...  \n",
       "98  Value for money❤️❤️\\nIts awesome mobile phone ...  \n",
       "99  Best budget Iphone till date ❤️ go for it guys...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final DataFrame.\n",
    "Apple_Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Flipkart website.\n",
    "url = 'https://www.flipkart.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using search code we find element by xpath and searching the word of sneakers.And clicking using click() function.\n",
    "search_bar = driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "search_bar.click()\n",
    "search_bar.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty sets .\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "Discount = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding elements brand,product_description,price and discount by xpath.\n",
    "for i in range(0,3):\n",
    "    brand = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    product_description = driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for i in product_description:\n",
    "        Product_Description.append(i.text)\n",
    "        \n",
    "    price = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    discount = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span[1]\")\n",
    "    for i in discount:\n",
    "        Discount.append(i.text)\n",
    "    driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chevit',\n",
       " 'India hub',\n",
       " 'CALCADOS',\n",
       " 'Shoes Bank',\n",
       " 'Robbie jones',\n",
       " 'KULP',\n",
       " 'ORICUM',\n",
       " 'PEHANOSA',\n",
       " 'Alfiya',\n",
       " 'Stefano Rads']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of Brand.\n",
    "Brand[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unique & Perfect Collection Combo Pack of 02 Shoes for ...',\n",
       " \"White Sneaker For Men's/Boy's Sneakers For Men\",\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Combo pack of 2 casual sneaker shoes for men Sneakers F...',\n",
       " 'Sneakers For Men',\n",
       " 'Shoes in Black Color Party wear/Outdoor/Casual Shoes Fo...',\n",
       " 'casual for men (blue 06) Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers Sneakers For Men']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of product_description.\n",
    "Product_Description[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹448',\n",
       " '₹389',\n",
       " '₹748',\n",
       " '₹349',\n",
       " '₹474',\n",
       " '₹399',\n",
       " '₹377',\n",
       " '₹499',\n",
       " '₹377',\n",
       " '₹242']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of Price.\n",
    "Price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['70% off',\n",
       " '87% off',\n",
       " '62% off',\n",
       " '65% off',\n",
       " '52% off',\n",
       " '60% off',\n",
       " '62% off',\n",
       " '50% off',\n",
       " '62% off',\n",
       " '65% off']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of discount.\n",
    "Discount[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame of Sneakers_Info.\n",
    "Sneakers_Info = pd.DataFrame({})\n",
    "Sneakers_Info['Brand'] = Brand[:100]\n",
    "Sneakers_Info['Product_Description'] = Product_Description[:100]\n",
    "Sneakers_Info['Price'] = Price[:100]\n",
    "Sneakers_Info['Discount'] = Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Unique &amp; Perfect Collection Combo Pack of 02 S...</td>\n",
       "      <td>₹448</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India hub</td>\n",
       "      <td>White Sneaker For Men's/Boy's Sneakers For Men</td>\n",
       "      <td>₹389</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CALCADOS</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹748</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shoes Bank</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Combo pack of 2 casual sneaker shoes for men S...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Birde</td>\n",
       "      <td>Acrux IDP Sneakers For Men</td>\n",
       "      <td>₹283</td>\n",
       "      <td>43% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>Combo Pack of 4 Latest Collection Stylish Casu...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>HOTSTYLE</td>\n",
       "      <td>Combo Pack of 2 Casual Shoes Sneakers For Men</td>\n",
       "      <td>₹283</td>\n",
       "      <td>43% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>restinfoot</td>\n",
       "      <td>Speed Set of 5 Pairs Sneakers Outdoors Casuals...</td>\n",
       "      <td>₹379</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹473</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                                Product_Description Price  \\\n",
       "0         Chevit  Unique & Perfect Collection Combo Pack of 02 S...  ₹448   \n",
       "1      India hub     White Sneaker For Men's/Boy's Sneakers For Men  ₹389   \n",
       "2       CALCADOS                                   Sneakers For Men  ₹748   \n",
       "3     Shoes Bank                                   Sneakers For Men  ₹349   \n",
       "4   Robbie jones  Combo pack of 2 casual sneaker shoes for men S...  ₹474   \n",
       "..           ...                                                ...   ...   \n",
       "95         Birde                         Acrux IDP Sneakers For Men  ₹283   \n",
       "96      ASTEROID  Combo Pack of 4 Latest Collection Stylish Casu...  ₹499   \n",
       "97      HOTSTYLE      Combo Pack of 2 Casual Shoes Sneakers For Men  ₹283   \n",
       "98    restinfoot  Speed Set of 5 Pairs Sneakers Outdoors Casuals...  ₹379   \n",
       "99        Chevit                                   Sneakers For Men  ₹473   \n",
       "\n",
       "   Discount  \n",
       "0   70% off  \n",
       "1   87% off  \n",
       "2   62% off  \n",
       "3   65% off  \n",
       "4   52% off  \n",
       "..      ...  \n",
       "95  43% off  \n",
       "96  75% off  \n",
       "97  43% off  \n",
       "98  62% off  \n",
       "99  73% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final DataFrame.\n",
    "Sneakers_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sneakers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Go to the link - https://www.myntra.com/shoes. Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black” and scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the myntra website.\n",
    "url = 'https://www.myntra.com/shoes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put the Price filter\n",
    "price_btn=driver.find_element_by_xpath('//ul[@class=\"price-list\"]/li[2]/label/div')\n",
    "price_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put the colour filter\n",
    "colour_button=driver.find_element_by_xpath('//li[@class=\"colour-listItem\"]/label/div')\n",
    "colour_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craeting empty set.\n",
    "Brand = []\n",
    "Description = []\n",
    "Price = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding elements of brand,description,price through driver by xpath.\n",
    "# Using for loop and appending text to empty sets.\n",
    "for i in range(0,2):\n",
    "    brand = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    description = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "    for i in description:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    price = driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//a[@rel='next']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nike',\n",
       " 'Nike',\n",
       " 'Skechers',\n",
       " 'Nike',\n",
       " 'Nike',\n",
       " 'Nike',\n",
       " 'Nike',\n",
       " 'Nike',\n",
       " 'Nike',\n",
       " 'UNDER ARMOUR']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of Brand.\n",
    "Brand[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Men ZOOM FREAK 2 Basketball',\n",
       " 'Unisex COSMIC UNITY Basketball',\n",
       " 'Men GO RUN 7+ Running Shoes',\n",
       " 'Men JORDAN DELTA Basketball',\n",
       " 'AIR ZOOM PEGASUS Running Shoes',\n",
       " 'Women PEGASUS 37 Running Shoes',\n",
       " 'Men KD13 EP Basketball Shoes',\n",
       " 'Men JOYRIDE Running Shoes',\n",
       " 'Women REACT Running Shoes',\n",
       " 'HOVR Sonic 3 Running Shoes']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of description.\n",
    "Description[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rs. 7206Rs. 10295(30% OFF)',\n",
       " 'Rs. 11470Rs. 13495(15% OFF)',\n",
       " 'Rs. 9999',\n",
       " 'Rs. 12495',\n",
       " 'Rs. 11495',\n",
       " 'Rs. 6996Rs. 9995(30% OFF)',\n",
       " 'Rs. 12995',\n",
       " 'Rs. 10496Rs. 14995(30% OFF)',\n",
       " 'Rs. 8396Rs. 11995(30% OFF)',\n",
       " 'Rs. 7699Rs. 10999(30% OFF)']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking 10 values of Price.\n",
    "Price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating into a Data Frame.\n",
    "Sneakers_Info = pd.DataFrame({})\n",
    "Sneakers_Info['Brand_name'] = Brand\n",
    "Sneakers_Info['Description'] = Description\n",
    "Sneakers_Info['Price'] = Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand_name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men ZOOM FREAK 2 Basketball</td>\n",
       "      <td>Rs. 7206Rs. 10295(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Unisex COSMIC UNITY Basketball</td>\n",
       "      <td>Rs. 11470Rs. 13495(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men GO RUN 7+ Running Shoes</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men JORDAN DELTA Basketball</td>\n",
       "      <td>Rs. 12495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>AIR ZOOM PEGASUS Running Shoes</td>\n",
       "      <td>Rs. 11495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RAPAWALK</td>\n",
       "      <td>Wide Width Leather Oxfords</td>\n",
       "      <td>Rs. 8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Women Peep Toe Heels</td>\n",
       "      <td>Rs. 7192Rs. 8990(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>FILA</td>\n",
       "      <td>Women Sneakers Casual Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Ruosh</td>\n",
       "      <td>Men Formal Leather Brogues</td>\n",
       "      <td>Rs. 7490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Women Leather Sneakers</td>\n",
       "      <td>Rs. 7699Rs. 10999(30% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand_name                     Description  \\\n",
       "0                   Nike     Men ZOOM FREAK 2 Basketball   \n",
       "1                   Nike  Unisex COSMIC UNITY Basketball   \n",
       "2               Skechers     Men GO RUN 7+ Running Shoes   \n",
       "3                   Nike     Men JORDAN DELTA Basketball   \n",
       "4                   Nike  AIR ZOOM PEGASUS Running Shoes   \n",
       "..                   ...                             ...   \n",
       "95              RAPAWALK      Wide Width Leather Oxfords   \n",
       "96  Heel & Buckle London            Women Peep Toe Heels   \n",
       "97                  FILA     Women Sneakers Casual Shoes   \n",
       "98                 Ruosh      Men Formal Leather Brogues   \n",
       "99             Cole Haan          Women Leather Sneakers   \n",
       "\n",
       "                          Price  \n",
       "0    Rs. 7206Rs. 10295(30% OFF)  \n",
       "1   Rs. 11470Rs. 13495(15% OFF)  \n",
       "2                      Rs. 9999  \n",
       "3                     Rs. 12495  \n",
       "4                     Rs. 11495  \n",
       "..                          ...  \n",
       "95                     Rs. 8850  \n",
       "96    Rs. 7192Rs. 8990(20% OFF)  \n",
       "97                     Rs. 7999  \n",
       "98                     Rs. 7490  \n",
       "99   Rs. 7699Rs. 10999(30% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Scraped Data Frame.\n",
    "Sneakers_Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Go to webpage https://www.amazon.in/\n",
    "1. Enter “Laptop” in the search field and then click the search icon.\n",
    "2. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes \n",
    "for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r\"C:\\Users\\Devu\\Downloads\\chromedriver_win32\\chromedriver\"\n",
    "driver=webdriver.Chrome(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of Amazon website.\n",
    "url = 'https://www.amazon.in/s?k=laptop&i=computers&rh=n%3A1375424031&dc&qid=1611511805&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting URL  by webdriver.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element of i7-button to filter by xpath and click the i7_button.\n",
    "i7_button = driver.find_element_by_xpath(\"//li[@aria-label='Intel Core i7']/span/a\")\n",
    "i7_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To put the filter for Intel core i9\n",
    "i9_button=driver.find_element_by_xpath(\"//li[@aria-label='Intel Core i9']/span/a\")\n",
    "i9_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty sets\n",
    "Title = []\n",
    "Rating = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"a20f4221-b606-43ac-b922-b300b06e44d1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"57acc5af-4995-4fac-a61a-51535588970c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"55ae9b1f-51b9-4a2e-b52a-331a0834b9b1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"a1f8bb60-a9e6-4454-bd2b-153aabefc7f1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"093bc4c4-b5e7-4496-96c4-5f770e267493\")>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding elements of title through driver by xpath.\n",
    "title = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASUS ZenBook Flip S OLED, Intel Evo Core i7-1165G7 11th Gen, 13.3-inch UHD Touch Thin and Light 2-in-1 Laptop (16GB/1TB SSD/Windows 10/Office 2019/Iris X Graphics/Jade Black/1.2 kg), UX371EA-HL701TS',\n",
       " 'Mi Notebook Horizon Edition 14 Intel Core i5-10210U 10th Gen 14-inch (35.56 cms) Thin and Light Laptop(8GB/512GB SSD/Windows 10/Nvidia MX350 2GB Graphics/Grey/1.35Kg), XMA1904-AR+Webcam',\n",
       " 'HP Pavilion (2021) Thin & Light 11th Gen Core i7 Laptop, 16 GB RAM, 1TB SSD, Iris Xe Graphics, 14\" (35.56cms) FHD Screen, Windows 10, MS Office, Backlit Keyboard (14-dv0058TU)',\n",
       " 'HP 14 Thin & Light 14\" (35.56cms) FHD Laptop (11th Gen Intel i7-1165G7/8GB/512GB SSD/Windows 10/MS Office 2019/Alexa Built-in/Pale Gold/1.47 kg), 14s-dr2007TU']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using for loop and appending the text to Title.\n",
    "for i in title:\n",
    "    Title.append(i.text)\n",
    "Title[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"391afc77-b4bb-4a20-bdbe-0f84c5a119e8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"aea45066-6b1c-4201-838b-86f6b258c133\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"8c4b31e3-d7c3-448b-a427-fdbe08cc7c6f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"d9e9421e-f881-4170-95e9-d2c3f9dfd8ee\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cba1fe8286bf24268135bea304b9879\", element=\"8b7e917d-290d-406f-985e-fe5bad6a7f4f\")>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding elements of price through driver by xpath.\n",
    "price = driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "price[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,45,490', '54,999', '84,990', '89,000']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using for loop and appending the text to Price empty set.\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "Price[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#locating Ratings\n",
    "urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")#collecting urls of all the laptop\n",
    "UR=[]\n",
    "for i in urls[:10]:\n",
    "    UR.append(i.get_attribute('href'))#getting the url of first 10 laptops\n",
    "for url in UR:#loop for every laptop in the list\n",
    "    driver.get(url)\n",
    "    try:                  #exception handling for nosuchelementexception\n",
    "        rate=driver.find_element_by_xpath(\"//span[@id='acrCustomerReviewText']\")#locating the ratingd link\n",
    "        rate.click()                                                      #click the rating link found\n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")#locating the rating\n",
    "        Rating.append(rating.text)#appending the rating in Rating.\n",
    "        \n",
    "    except NoSuchElementException   as e:\n",
    "        Rating.append(\"NO rating\") # Used to get if there is no rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.6 out of 5',\n",
       " '4.4 out of 5',\n",
       " '4.5 out of 5',\n",
       " '4.7 out of 5',\n",
       " '4.1 out of 5',\n",
       " '3.1 out of 5',\n",
       " '3.8 out of 5',\n",
       " '4.2 out of 5',\n",
       " '4.3 out of 5',\n",
       " 'NO rating']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Rating.\n",
    "Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame.\n",
    "Laptop_Amazon = pd.DataFrame({})\n",
    "Laptop_Amazon['Title'] = Title[:10]\n",
    "Laptop_Amazon['Rating'] = Rating[:10]\n",
    "Laptop_Amazon['Price'] = Price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS ZenBook Flip S OLED, Intel Evo Core i7-11...</td>\n",
       "      <td>4.6 out of 5</td>\n",
       "      <td>1,45,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i5-1...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>54,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP Pavilion (2021) Thin &amp; Light 11th Gen Core ...</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP 14 Thin &amp; Light 14\" (35.56cms) FHD Laptop (...</td>\n",
       "      <td>4.7 out of 5</td>\n",
       "      <td>89,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion Gaming 10th Gen Intel Core i7 Proc...</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Life Digital Laptop 15.6-inch (39.62 cms) (Int...</td>\n",
       "      <td>3.1 out of 5</td>\n",
       "      <td>26,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS ROG Zephyrus Duo 15, 15.6\" 4K UHD, Intel ...</td>\n",
       "      <td>3.8 out of 5</td>\n",
       "      <td>2,69,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Yoga 7 11th Gen Intel Core i7 14\" (35.5...</td>\n",
       "      <td>4.2 out of 5</td>\n",
       "      <td>97,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo IdeaPad Gaming 3 10th Gen Intel Core i7...</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "      <td>77,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Renewed) Dell G7 7500 15.6inch FHD 300 Hz Dis...</td>\n",
       "      <td>NO rating</td>\n",
       "      <td>1,70,587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Rating     Price\n",
       "0  ASUS ZenBook Flip S OLED, Intel Evo Core i7-11...  4.6 out of 5  1,45,490\n",
       "1  Mi Notebook Horizon Edition 14 Intel Core i5-1...  4.4 out of 5    54,999\n",
       "2  HP Pavilion (2021) Thin & Light 11th Gen Core ...  4.5 out of 5    84,990\n",
       "3  HP 14 Thin & Light 14\" (35.56cms) FHD Laptop (...  4.7 out of 5    89,000\n",
       "4  HP Pavilion Gaming 10th Gen Intel Core i7 Proc...  4.1 out of 5    86,990\n",
       "5  Life Digital Laptop 15.6-inch (39.62 cms) (Int...  3.1 out of 5    26,990\n",
       "6  ASUS ROG Zephyrus Duo 15, 15.6\" 4K UHD, Intel ...  3.8 out of 5  2,69,990\n",
       "7  Lenovo Yoga 7 11th Gen Intel Core i7 14\" (35.5...  4.2 out of 5    97,990\n",
       "8  Lenovo IdeaPad Gaming 3 10th Gen Intel Core i7...  4.3 out of 5    77,990\n",
       "9  (Renewed) Dell G7 7500 15.6inch FHD 300 Hz Dis...     NO rating  1,70,587"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final DataFrame.\n",
    "Laptop_Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"laptops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
